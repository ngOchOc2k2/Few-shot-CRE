{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-27 14:08:16.626452: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import random\n",
    "from sampler import data_sampler\n",
    "from config import Config\n",
    "import torch\n",
    "from model.bert_encoder import Bert_Encoder, Bert_LoRa\n",
    "from model.dropout_layer import Dropout_Layer\n",
    "from model.classifier import Softmax_Layer, Proto_Softmax_Layer\n",
    "from data_loader import get_data_loader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import collections\n",
    "from copy import deepcopy\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "from process import sequence_data_sampler_bert_prompt, data_sampler_bert_prompt_deal_first_task\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from peft import get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class args:\n",
    "    batch_size = 64\n",
    "    gradient_accumulation_steps = 4\n",
    "    total_round = 6\n",
    "    drop_out = 0.5\n",
    "    num_workers = 2\n",
    "    step1_epochs = 10\n",
    "    step2_epochs = 10\n",
    "    num_protos = 1\n",
    "    device = 'cuda'\n",
    "    seed = 100\n",
    "    max_grad_norm = 10\n",
    "    task_length = 8\n",
    "    kl_temp = 2\n",
    "    temp = 0.1\n",
    "\n",
    "    bert_path = 'bert-base-uncased'\n",
    "    max_length = 256\n",
    "    vocab_size = 30522\n",
    "    marker_size = 4\n",
    "    pattern = 'entity_marker'\n",
    "    encoder_output_size = 768\n",
    "    lora = True\n",
    "    save_lora = './Checkpoint/LoRA_2'\n",
    "\n",
    "    drop_p = 0.1\n",
    "    f_pass = 10\n",
    "    kappa_neg = 0.03\n",
    "    kappa_pos = 0.05\n",
    "\n",
    "    T_mult = 1\n",
    "    rewarm_epoch_num = 2\n",
    "    # StepLR\n",
    "    decay_rate = 0.9\n",
    "    decay_steps = 800\n",
    "    task = 'tacred'\n",
    "    shot = 10\n",
    "    \n",
    "config = args\n",
    "\n",
    "\n",
    "config.device = torch.device(config.device)\n",
    "config.n_gpu = torch.cuda.device_count()\n",
    "config.batch_size_per_step = int(config.batch_size / config.gradient_accumulation_steps)\n",
    "\n",
    "config.task = args.task\n",
    "config.shot = args.shot\n",
    "config.step1_epochs = 10\n",
    "config.step2_epochs = 15\n",
    "config.step3_epochs = 20\n",
    "config.temperature = 0.08\n",
    "\n",
    "\n",
    "config.relation_file = \"/home/luungoc/Continual Learning/data/tacred/relation_name.txt\"\n",
    "config.rel_index = \"/home/luungoc/Continual Learning/data/tacred/rel_index.npy\"\n",
    "config.rel_feature = \"/home/luungoc/Continual Learning/data/tacred/rel_feature.npy\"\n",
    "config.num_of_relation = 41\n",
    "config.rel_cluster_label = \"/home/luungoc/Continual Learning/data/tacred/CFRLdata_10_100_10_5/rel_cluster_label_0.npy\"\n",
    "config.training_file = \"/home/luungoc/Continual Learning/data/tacred/CFRLdata_10_100_10_5/train_0.txt\"\n",
    "config.valid_file = \"/home/luungoc/Continual Learning/data/tacred/CFRLdata_10_100_10_5/valid_0.txt\"\n",
    "config.test_file = \"/home/luungoc/Continual Learning/data/tacred/CFRLdata_10_100_10_5/test_0.txt\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 0 6 2 5 1 4 3]\n"
     ]
    }
   ],
   "source": [
    "for rou in range(1):\n",
    "    test_cur = []\n",
    "    test_total = []\n",
    "    random.seed(100 + rou*10)\n",
    "    sampler = data_sampler(config=config, seed=100 + rou*10)\n",
    "    id2rel = sampler.id2rel\n",
    "    rel2id = sampler.rel2id\n",
    "    id2sentence = sampler.get_id2sent()\n",
    "    \n",
    "\n",
    "    # print(f\"Param trainable: {sum(p.numel() for p in encoder.parameters() if p.requires_grad)}\")\n",
    "    \n",
    "    num_class = len(sampler.id2rel)\n",
    "\n",
    "    memorized_samples = {}\n",
    "    memory = collections.defaultdict(list)\n",
    "    history_relations = []\n",
    "    history_data = []\n",
    "    prev_relations = []\n",
    "    classifier = None\n",
    "    prev_classifier = None\n",
    "    prev_encoder = None\n",
    "    prev_dropout_layer = None\n",
    "    relation_standard = {}\n",
    "    forward_accs = []\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "batch = 1\n",
    "rel_index = np.load(config.rel_index)\n",
    "rel_cluster_label = np.load(config.rel_cluster_label)\n",
    "cluster_to_labels = {}\n",
    "for index, i in enumerate(rel_index):\n",
    "    if rel_cluster_label[index] in cluster_to_labels.keys():\n",
    "        cluster_to_labels[rel_cluster_label[index]].append(i-1)\n",
    "    else:\n",
    "        cluster_to_labels[rel_cluster_label[index]] = [i-1]\n",
    "\n",
    "task_length = 7\n",
    "shuffle_index_old = list(range(task_length - 1))\n",
    "random.shuffle(shuffle_index_old)\n",
    "shuffle_index_old = np.argsort(shuffle_index_old)\n",
    "shuffle_index = np.insert(shuffle_index_old, 0, task_length - 1)\n",
    "\n",
    "\n",
    "\n",
    "indexs = cluster_to_labels[shuffle_index[batch]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25, 15, 35, 37, 38]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------\n",
      "length seen relation: 100\n",
      "------------------------------------------------------------------------------------------\n",
      "length seen relation: 9\n",
      "------------------------------------------------------------------------------------------\n",
      "length seen relation: 5\n",
      "------------------------------------------------------------------------------------------\n",
      "length seen relation: 52\n",
      "------------------------------------------------------------------------------------------\n",
      "length seen relation: 44\n",
      "------------------------------------------------------------------------------------------\n",
      "length seen relation: 37\n",
      "------------------------------------------------------------------------------------------\n",
      "length seen relation: 81\n",
      "------------------------------------------------------------------------------------------\n",
      "length seen relation: 100\n"
     ]
    }
   ],
   "source": [
    "for steps, (training_data, valid_data, test_data, current_relations, historic_test_data, seen_relations) in enumerate(sampler):\n",
    "    print('---'*30 )\n",
    "    total = 0\n",
    "    for item in current_relations:\n",
    "        total += len(training_data[item])\n",
    "    # print('current training data num: ' + str(total))\n",
    "    # print(f'length training data: {len(training_data)}')\n",
    "    # print(f'length valid data: {len(valid_data)}')\n",
    "    # print(f'length test data: {len(test_data)}')\n",
    "    \n",
    "    # print(f'current relation: {current_relations}')\n",
    "    # print(f'length historic_test_data: {historic_test_data.keys()}')\n",
    "    print(f'length seen relation: {len(test_data[current_relations[0]])}')\n",
    "    # print(training_data[current_relations[0]][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "with open('/home/luungoc/Continual Learning/data/tacred/CFRLdata_10_100_10_5/test_0.txt') as file_in:\n",
    "    for line in file_in:\n",
    "        items = line.strip().split('\\t')\n",
    "        if (len(items[0]) > 0):\n",
    "            relation_ix = int(items[0])\n",
    "            if items[1] != 'noNegativeAnswer':\n",
    "                candidate_ixs = [int(ix) for ix in items[1].split()]\n",
    "                sentence = items[2].split('\\n')[0]\n",
    "                headent = items[3]\n",
    "                headidx = [int(ix) for ix in items[4].split()]\n",
    "                tailent = items[5]\n",
    "                tailidx = [int(ix) for ix in items[6].split()]\n",
    "                headid = items[7]\n",
    "                tailid = items[8]\n",
    "                samples.append([relation_ix, candidate_ixs, sentence, headent, headidx, tailent, tailidx, headid, tailid])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
