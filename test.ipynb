{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "from sampler import data_sampler\n",
    "from config import Config\n",
    "import torch\n",
    "from model.bert_encoder import Bert_Encoder, Bert_LoRa\n",
    "from model.dropout_layer import Dropout_Layer\n",
    "from model.classifier import Softmax_Layer, Proto_Softmax_Layer\n",
    "from data_loader import get_data_loader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import collections\n",
    "from copy import deepcopy\n",
    "import os\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from peft import get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class args:\n",
    "    batch_size = 64\n",
    "    gradient_accumulation_steps = 4\n",
    "    total_round = 6\n",
    "    drop_out = 0.5\n",
    "    num_workers = 2\n",
    "    step1_epochs = 10\n",
    "    step2_epochs = 10\n",
    "    num_protos = 1\n",
    "    device = 'cuda'\n",
    "    seed = 100\n",
    "    max_grad_norm = 10\n",
    "    task_length = 8\n",
    "    kl_temp = 2\n",
    "    temp = 0.1\n",
    "\n",
    "    bert_path = 'bert-base-uncased'\n",
    "    max_length = 256\n",
    "    vocab_size = 30522\n",
    "    marker_size = 4\n",
    "    pattern = 'entity_marker'\n",
    "    encoder_output_size = 768\n",
    "    lora = True\n",
    "    save_lora = './Checkpoint/LoRA_2'\n",
    "\n",
    "    drop_p = 0.1\n",
    "    f_pass = 10\n",
    "    kappa_neg = 0.03\n",
    "    kappa_pos = 0.05\n",
    "\n",
    "    T_mult = 1\n",
    "    rewarm_epoch_num = 2\n",
    "    # StepLR\n",
    "    decay_rate = 0.9\n",
    "    decay_steps = 800\n",
    "    task = 'tacred'\n",
    "    shot = 10\n",
    "    \n",
    "config = args\n",
    "\n",
    "\n",
    "config.device = torch.device(config.device)\n",
    "config.n_gpu = torch.cuda.device_count()\n",
    "config.batch_size_per_step = int(config.batch_size / config.gradient_accumulation_steps)\n",
    "\n",
    "config.task = args.task\n",
    "config.shot = args.shot\n",
    "config.step1_epochs = 10\n",
    "config.step2_epochs = 15\n",
    "config.step3_epochs = 20\n",
    "config.temperature = 0.08\n",
    "\n",
    "\n",
    "config.relation_file = \"/home/luungoc/Continual Learning/data/tacred/relation_name.txt\"\n",
    "config.rel_index = \"/home/luungoc/Continual Learning/data/tacred/rel_index.npy\"\n",
    "config.rel_feature = \"/home/luungoc/Continual Learning/data/tacred/rel_feature.npy\"\n",
    "config.num_of_relation = 41\n",
    "config.rel_cluster_label = \"/home/luungoc/Continual Learning/data/tacred/CFRLdata_10_100_10_5/rel_cluster_label_0.npy\"\n",
    "config.training_file = \"/home/luungoc/Continual Learning/data/tacred/CFRLdata_10_100_10_5/train_0.txt\"\n",
    "config.valid_file = \"/home/luungoc/Continual Learning/data/tacred/CFRLdata_10_100_10_5/valid_0.txt\"\n",
    "config.test_file = \"/home/luungoc/Continual Learning/data/tacred/CFRLdata_10_100_10_5/test_0.txt\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rou in range(1):\n",
    "    test_cur = []\n",
    "    test_total = []\n",
    "    random.seed(100 + rou*10)\n",
    "    sampler = data_sampler(config=config, seed=100 + rou*10)\n",
    "    id2rel = sampler.id2rel\n",
    "    rel2id = sampler.rel2id\n",
    "    id2sentence = sampler.get_id2sent()\n",
    "    \n",
    "\n",
    "    # print(f\"Param trainable: {sum(p.numel() for p in encoder.parameters() if p.requires_grad)}\")\n",
    "    \n",
    "    num_class = len(sampler.id2rel)\n",
    "\n",
    "    memorized_samples = {}\n",
    "    memory = collections.defaultdict(list)\n",
    "    history_relations = []\n",
    "    history_data = []\n",
    "    prev_relations = []\n",
    "    classifier = None\n",
    "    prev_classifier = None\n",
    "    prev_encoder = None\n",
    "    prev_dropout_layer = None\n",
    "    relation_standard = {}\n",
    "    forward_accs = []\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for steps, (training_data, valid_data, test_data, current_relations, historic_test_data, seen_relations) in enumerate(sampler):\n",
    "    print('---'*30 )\n",
    "    total = 0\n",
    "    for item in current_relations:\n",
    "        total += len(training_data[item])\n",
    "    print('current training data num: ' + str(total))\n",
    "    print(f'length training data: {len(training_data)}')\n",
    "    print(f'length valid data: {len(valid_data)}')\n",
    "    print(f'length test data: {len(test_data)}')\n",
    "    \n",
    "    print(f'current relation: {current_relations}')\n",
    "    print(f'length historic_test_data: {len(historic_test_data)}')\n",
    "    print(f'length seen relation: {len(seen_relations)}')\n",
    "    print(training_data[current_relations[0]][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "with open('/home/luungoc/Continual Learning/data/tacred/CFRLdata_10_100_10_5/test_0.txt') as file_in:\n",
    "    for line in file_in:\n",
    "        items = line.strip().split('\\t')\n",
    "        if (len(items[0]) > 0):\n",
    "            relation_ix = int(items[0])\n",
    "            if items[1] != 'noNegativeAnswer':\n",
    "                candidate_ixs = [int(ix) for ix in items[1].split()]\n",
    "                sentence = items[2].split('\\n')[0]\n",
    "                headent = items[3]\n",
    "                headidx = [int(ix) for ix in items[4].split()]\n",
    "                tailent = items[5]\n",
    "                tailidx = [int(ix) for ix in items[6].split()]\n",
    "                headid = items[7]\n",
    "                tailid = items[8]\n",
    "                samples.append([relation_ix, candidate_ixs, sentence, headent, headidx, tailent, tailidx, headid, tailid])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24,\n",
       " [13, 14, 25, 36, 8, 10, 41, 5, 28, 35, 24],\n",
       " \"campus christian chaplains and st. david 's lenten study for 2008 led by wayne holst :\",\n",
       " 'wayne holst',\n",
       " [13, 14],\n",
       " 'christian',\n",
       " [1],\n",
       " 'HHH',\n",
       " 'TTT']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
